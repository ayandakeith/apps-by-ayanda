{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An A-Z of Google Cloud Speech-to-Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to my quick guide to audio file transcription. If you are a beginner in th python or cloud world, you are probably familiar with how frustrating it can be to find the appropriate information to build your application in python. I have written this with you in mind. I hope you will find this useful for your first speech-to-text project.\n",
    "\n",
    "<p> To get started you will need the following:<br>\n",
    "1) Jupyter Notebook<br>\n",
    "2)  <a href=\"https://cloud.google.com/\" target=\"_blank\">A Google Cloud Account</a> - If you are new at this, start with the free account. After creating the account, go to your <a href=\"https://cloud.google.com/speech-to-text/docs/quickstart-client-libraries?hl=en_US\" target=\"_blank\">Google Cloud Console</a> and set up a project and install the client library.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Audio Files\n",
    "<p>\n",
    "- Google Cloud's speech-to-text API accepts various audio file formats. However, I found that my .m4a files could not be processed so I converted them to .wav. You should preferably save your files in .wav<br>\n",
    "- Audio files can be transcribed directly from a local folder or when uploaded into a <a href=\"https://console.cloud.google.com/storage\" target=\"_blank\">bucket</a>. (You will need to create a bucket first)<br>\n",
    "- I also found that it was impossible to transcribe long audio files using a local file on the free account. So, you are better off uploading into a bucket. Although, there is no harm in trying to transcribe a local file first. But just in case, I have included programs for the two different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) Import all the modules you will need\n",
    "from google.cloud import speech_v1p1beta1\n",
    "from google.cloud.speech_v1p1beta1 import enums\n",
    "from google.cloud import speech_v1\n",
    "from google.cloud.speech_v1 import enums\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayandadube/Dropbox/Python Real World Applications/Class of 2000 Legacy Meetings'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(2) Check your current working directory path and verify that your project .json file is there.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_Aug_pt1.wav\r\n",
      "29_Aug_pt2.wav\r\n",
      "29_August_2020_Zoom_Call_Pt1.ipynb\r\n",
      "An A-Z of Google Cloud Speech-to-Text.ipynb\r\n",
      "\u001b[34mAug 29 Zoom Call\u001b[m\u001b[m/\r\n",
      "My First Project-04d24feaec4b.json\r\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your GOOGLE_APPLICATION_CREDENTIALS environment variable in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"your-json- file-path-with-filename.json\"\n",
    "#eg. '/Users/ayandadube/Dropbox/Python Real World Applications/Class of 2000 Legacy Meetings/My First Project-04d24feaec4b.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing files\n",
    "Google Cloud provides you with <a href=\"https://cloud.google.com/speech-to-text/docs/async-recognize?hl=en_US\" target=\"_blank\">code</a> in various languages including Python, C#, Java, Node.js and many others. Below I have set up the programs for transcription using local files and Google Cloud Storage Files. Personally, I prefer transcribing using Google Cloud Storage Files. It generally seems less error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing long audio files using a local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognize(audio_file):\n",
    "    \"\"\"\n",
    "    Performs synchronous speech recognition on an audio file\n",
    "    Args:\n",
    "      audio_file URI for audio file.\n",
    "      you can define the audio file inside the function or outside it. My opinion - define it outside the function since you are likely to transcribe different files.\n",
    "    audio_file = \"/Users/ayandadube/Dropbox/Python Real World Applications/Class of 2000 Legacy Meetings/29_Aug_pt1.wav\"\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech_v1p1beta1.SpeechClient()\n",
    "   \n",
    "    # The language of the supplied audio\n",
    "    language_code = \"en-US\"\n",
    "\n",
    "    # Sample rate in Hertz of the audio data sent\n",
    "    sample_rate_hertz = 44100\n",
    "\n",
    "    # Encoding of audio data sent. This sample sets this explicitly.\n",
    "    # This field is optional for FLAC and WAV audio formats.\n",
    "    encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "    config = {\n",
    "        \"language_code\": language_code,\n",
    "        \"sample_rate_hertz\": sample_rate_hertz,\n",
    "        \"encoding\": encoding,\n",
    "    }\n",
    "    \n",
    "    with io.open(audio_file, \"rb\") as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    audio = {\"content\": content}\n",
    "    \n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print(u\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "\n",
    "    for result in response.results:\n",
    "        # First alternative is the most probable result\n",
    "        alternative = result.alternatives[0]\n",
    "        print(u\"Transcript: {}\".format(alternative.transcript))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"/Users/ayandadube/Dropbox/Python Real World Applications/Class of 2000 Legacy Meetings/29_Aug_pt1.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognize(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing long audio files using Google Cloud Storage file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_recognize(storage_uri):\n",
    "    \"\"\"\n",
    "    Transcribe long audio file from Cloud Storage using asynchronous speech\n",
    "    recognition\n",
    "\n",
    "    Args:\n",
    "      storage_uri URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]\n",
    "    \"\"\"\n",
    "\n",
    "    client = speech_v1.SpeechClient()\n",
    "\n",
    "    # storage_uri = 'gs://cloud-samples-data/speech/brooklyn_bridge.raw'\n",
    "\n",
    "    # Sample rate in Hertz of the audio data sent\n",
    "    sample_rate_hertz = 32000\n",
    "\n",
    "    # The language of the supplied audio\n",
    "    language_code = \"en-US\"\n",
    "\n",
    "    # Encoding of audio data sent. This sample sets this explicitly.\n",
    "    # This field is optional for FLAC and WAV audio formats.\n",
    "    encoding = enums.RecognitionConfig.AudioEncoding.LINEAR16\n",
    "    config = {\n",
    "        \"sample_rate_hertz\": sample_rate_hertz,\n",
    "        \"language_code\": language_code,\n",
    "        \"encoding\": encoding,\n",
    "    }\n",
    "    audio = {\"uri\": storage_uri}\n",
    "\n",
    "    operation = client.long_running_recognize(config, audio)\n",
    "\n",
    "    print(u\"Waiting for operation to complete...\")\n",
    "    response = operation.result()\n",
    "\n",
    "    for result in response.results:\n",
    "        # First alternative is the most probable result\n",
    "        alternative = result.alternatives[0]\n",
    "        print(u\"Transcript: {}\".format(alternative.transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_uri = 'gs://op-2000-legacy/29_Aug_pt1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognize(storage_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
